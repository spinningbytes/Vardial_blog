{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will first load the data into the directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based in  https://github.com/ynop/audiomate/blob/master/audiomate/utils/download.py\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import requests\n",
    "from collections import Counter\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def download_file(url, target_path=None):\n",
    "    \"\"\"\n",
    "    Download the file from the given `url` and store it at `target_path`.\n",
    "    \"\"\"\n",
    "    if target_path is None:\n",
    "        target_path = os.path.join(\"data\",url.split(\"/\")[-1])\n",
    "\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    with open(target_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return target_path\n",
    "\n",
    "def extract_zip(zip_path, target_folder):\n",
    "    \"\"\"\n",
    "    Extract the content of the zip-file at `zip_path` into `target_folder`.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path) as archive:\n",
    "        archive.extractall(target_folder)\n",
    "\n",
    "\n",
    "\n",
    "def get_data(fname=None, all=1):\n",
    "    \n",
    "    #sstopwords=set(stopwords.words('german'))\n",
    "    if fname is None:        \n",
    "        fname = \"./data/train.txt\"\n",
    "        \n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    #split the lines into text and labels and save separately\n",
    "    with open(fname) as f1:\n",
    "        for l1 in f1:\n",
    "            ws,lab = l1.split(\"\\t\")\n",
    "            texts.append(ws)\n",
    "            labels.append(lab.strip())\n",
    "\n",
    "    #character embeddings\n",
    "\n",
    "    #words = word_tokenize(words_raw)\n",
    "    tokensents = [tk.lower().split() for tk in texts]\n",
    "    words = set([word for tokens in tokensents for word in tokens])\n",
    "\n",
    "    chars = list(' '.join(words))\n",
    "\n",
    "    char_counts = Counter(chars)\n",
    "    labels_dict = {}\n",
    "    labels_nr = []\n",
    "    nums = set()\n",
    "    for i1,lab in enumerate(labels):\n",
    "        #this step will be explained later and means, that setences with maximum of only 2 tokens should\n",
    "        # be ignored\n",
    "        if all==0 and len(tokensents[i1])<=2:\n",
    "            continue\n",
    "        nums.add(i1)\n",
    "        if lab in labels_dict:\n",
    "            labels_nr.append(labels_dict[lab])\n",
    "        else:\n",
    "            labels_dict[lab] = len(labels_dict)\n",
    "            labels_nr.append(labels_dict[lab])\n",
    "    tokensents = [tk for i1,tk in enumerate(tokensents) if i1 in nums]\n",
    "    return labels, labels_nr, labels_dict, tokensents, words, chars, char_counts\n",
    "\n",
    "\n",
    "\n",
    "fname=\"https://scholar.harvard.edu/files/malmasi/files/vardial2018-gdi-training.zip\"\n",
    "target_path = download_file(fname)\n",
    "extract_zip( target_path, \"./data/\")\n",
    "\n",
    "labels_train, labels_nr_train, labels_dict_train,sents_train_raw, words_train, chars_train, char_counts_train = get_data()\n",
    "labels_dev_dev, labels_nr_dev, labels_dict_dev,sents_dev_raw, words_dev, chars_dev, char_counts_dev = get_data(\"./data/dev.txt\")\n",
    "\n",
    "sents_train= [\" \".join(tk).lower() for tk in sents_train_raw]\n",
    "sents_dev= [\" \".join(tk).lower() for tk in sents_dev_raw]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try to classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF weighted 0.6100404138001064\n",
      "SVM TF macro 0.6036412574311785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "n_features=20000\n",
    "\n",
    "clf_svm = LinearSVC(random_state=0,C=1)\n",
    "tf_vectorizer = CountVectorizer( min_df=2,\n",
    "                                max_features=n_features)\n",
    "\n",
    "\n",
    "tf_train = tf_vectorizer.fit_transform(sents_train)\n",
    "\n",
    "tf_dev = tf_vectorizer.transform(sents_dev)\n",
    "\n",
    "clf_svm.fit(tf_train,labels_nr_train)\n",
    "\n",
    "print(\"SVM TF weighted\",f1_score(clf_svm.predict(tf_dev),labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF macro\",f1_score(clf_svm.predict(tf_dev),labels_nr_dev, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems pretty already pretty good compared to last years resul, where the best was 0.66 (weighted F-1 score, https://www.aclweb.org/anthology/W/W17/W17-1201.pdf ).\n",
    "But we can do much better using just a simple trick: We skip the one and two words sentences. This is because they are bad for training the classifier. Why they are bad, mostly for two reasons, too few features (only one and two words), therefore difficult to differentiate, and also very confusing labels. So same sentence being labelled with two different labels. Let's see how good this works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF weighted 0.6438060442167858\n",
      "SVM TF macro 0.6366767077208305\n"
     ]
    }
   ],
   "source": [
    "labels_train, labels_nr_train, labels_dict_train,sents_train_raw, words_train, chars_train, char_counts_train = get_data(all=0)\n",
    "labels_dev_dev, labels_nr_dev, labels_dict_dev,sents_dev_raw, words_dev, chars_dev, char_counts_dev = get_data(\"data/dev.txt\",all=0)\n",
    "\n",
    "sents_train= [\" \".join(tk).lower() for tk in sents_train_raw]\n",
    "sents_dev= [\" \".join(tk).lower() for tk in sents_dev_raw]\n",
    "\n",
    "tf_train = tf_vectorizer.fit_transform(sents_train)\n",
    "\n",
    "tf_dev = tf_vectorizer.transform(sents_dev)\n",
    "\n",
    "clf_svm.fit(tf_train,labels_nr_train)\n",
    "\n",
    "print(\"SVM TF weighted\",f1_score(clf_svm.predict(tf_dev),labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF macro\",f1_score(clf_svm.predict(tf_dev),labels_nr_dev, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, 3 points which gives already fourth place compared to last year. Surprinsingly, we are just counting the words appearing in the sentences. Normally, SVMs work best with normalized data, but that's is for another tutorial.\n",
    "Let's see if there is a weighting scheme better than term frequency. Normally Term-Frequency Inverse-Document-Frequency (TF-IDF) works better for document classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF weighted 0.659225631519502\n",
      "SVM TF-IDF macro 0.6514922794527852\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer( min_df=2,\n",
    "                                 norm=\"l2\",\n",
    "                                   max_features=n_features)\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(sents_train)\n",
    "tfidf_dev = tfidf_vectorizer.transform(sents_dev)\n",
    "\n",
    "clf_svm.fit(tfidf_train,labels_nr_train)\n",
    "\n",
    "\n",
    "print(\"SVM TF-IDF weighted\",f1_score(clf_svm.predict(tfidf_dev),labels_nr_dev, average=\"weighted\"))\n",
    "\n",
    "print(\"SVM TF-IDF macro\",f1_score(clf_svm.predict(tfidf_dev),labels_nr_dev, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would made third place, almost second. But this is with SVM do others classifiers perform well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF TF weighted 0.4885197964113094\n",
      "RF TF macro 0.4221673549708851\n",
      "RF TF-IDF weighted 0.4893916483789577\n",
      "RF TF-IDF macro 0.4228223976071996\n",
      "Bagging TF weighted 0.46183188110927015\n",
      "Bagging TF macro 0.3985855877443282\n",
      "Bagging TFIDF weighted 0.5021817720653874\n",
      "Bagging TFIDF macro 0.3917232063144712\n",
      "Bagging TF+TFIDF weighted 0.47146645251840025\n",
      "Bagging TF+TFIDF macro 0.39716687771209946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=0)\n",
    "\n",
    "clf.fit(tf_train,labels_nr_train)\n",
    "\n",
    "\n",
    "print(\"RF TF weighted\",f1_score(clf.predict(tf_dev),labels_nr_dev, average=\"weighted\"))\n",
    "print(\"RF TF macro\",f1_score(clf.predict(tf_dev),labels_nr_dev, average=\"macro\"))\n",
    "\n",
    "\n",
    "clf.fit(tfidf_train,labels_nr_train)\n",
    "\n",
    "print(\"RF TF-IDF weighted\",f1_score(clf.predict(tfidf_dev),labels_nr_dev, average=\"weighted\"))\n",
    "print(\"RF TF-IDF macro\",f1_score(clf.predict(tfidf_dev),labels_nr_dev, average=\"macro\"))\n",
    "\n",
    "\n",
    "bagging = BaggingClassifier( RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=0),\n",
    "                             max_samples=0.5, max_features=0.5)\n",
    "bagging.fit(tf_train,labels_nr_train)\n",
    "\n",
    "\n",
    "print(\"Bagging TF weighted\",f1_score(bagging.predict(tf_dev),labels_nr_dev, average=\"weighted\"))\n",
    "print(\"Bagging TF macro\",f1_score(bagging.predict(tf_dev),labels_nr_dev, average=\"macro\"))\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier( RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=0),\n",
    "                             max_samples=0.5, max_features=0.5)\n",
    "bagging.fit(tfidf_train,labels_nr_train)\n",
    "\n",
    "\n",
    "print(\"Bagging TFIDF weighted\",f1_score(bagging.predict(tfidf_dev),labels_nr_dev, average=\"weighted\"))\n",
    "print(\"Bagging TFIDF macro\",f1_score(bagging.predict(tfidf_dev),labels_nr_dev, average=\"macro\"))\n",
    "\n",
    "bagging = BaggingClassifier( RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=0),\n",
    "                             max_samples=0.5, max_features=0.5)\n",
    "\n",
    "bagging.fit(scipy.sparse.hstack([tf_train,tfidf_train]).tocsr(),labels_nr_train)\n",
    "\n",
    "\n",
    "print(\"Bagging TF+TFIDF weighted\",f1_score(bagging.predict(scipy.sparse.hstack([tf_dev,tfidf_dev]).tocsr()),labels_nr_dev, average=\"weighted\"))\n",
    "print(\"Bagging TF+TFIDF macro\",f1_score(bagging.predict(scipy.sparse.hstack([tf_dev,tfidf_dev]).tocsr()),labels_nr_dev, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging and RF sometimes are good sometimes are bad, also using SVM decreases bagging the performance.\n",
    "We now try with n-ngrams of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF tf char ngram 7 0.6558271945540242\n",
      "SVM TF-IDF tf char ngram macro 0.6468589036370523\n"
     ]
    }
   ],
   "source": [
    "# additionally use normlized tfidf char bigrams \n",
    "tf_vectorizer_char_ngram = CountVectorizer( min_df=2,\n",
    "                                               analyzer=\"char\",\n",
    "                                   max_features=n_features, ngram_range=(1, 7))\n",
    "tf_train_char_ngram = tf_vectorizer_char_ngram.fit_transform(sents_train)\n",
    "tf_dev_char_ngram = tf_vectorizer_char_ngram.transform(sents_dev)\n",
    "\n",
    "clf_svm.fit(tf_train_char_ngram,labels_nr_train)\n",
    "preds=clf_svm.predict(tf_dev_char_ngram)\n",
    "print(\"SVM TF-IDF tf char ngram 7\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF-IDF tf char ngram macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF tf char ngram 7 0.6774906696759666\n",
      "SVM TF-IDF tf char ngram macro 0.6658621351916394\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# additionally use normlized tfidf char bigrams \n",
    "tfidf_vectorizer_char_ngram = TfidfVectorizer( min_df=2,\n",
    "                                 norm=\"l2\",\n",
    "                                               analyzer=\"char\",\n",
    "                                   max_features=n_features, ngram_range=(1, 7))\n",
    "tfidf_train_char_ngram = tfidf_vectorizer_char_ngram.fit_transform(sents_train)\n",
    "tfidf_dev_char_ngram = tfidf_vectorizer_char_ngram.transform(sents_dev)\n",
    "\n",
    "clf_svm.fit(tfidf_train_char_ngram,labels_nr_train)\n",
    "preds=clf_svm.predict(tfidf_dev_char_ngram)\n",
    "print(\"SVM TF-IDF tf char ngram 7\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF-IDF tf char ngram macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, now we would win the competitions from 2017. Here we see also tfidf works well also for sentences and chars. But this was out of the box, can we do better bigrams? Why bigrams, because we are looking to features which are  phonetically similar and similarly structured, not quite perfect match. Also the number of aas and ee might give good hints. I'll show you why the tfidf_vectorizer with char n-gram could do even better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' a',\n",
       " ' a ',\n",
       " ' a d',\n",
       " ' a d ',\n",
       " ' a de',\n",
       " ' a de ',\n",
       " ' aa',\n",
       " ' aa ',\n",
       " ' aab',\n",
       " ' aabe',\n",
       " ' aaf',\n",
       " ' aafa',\n",
       " ' aafan',\n",
       " ' aafang',\n",
       " ' aag',\n",
       " ' aagf',\n",
       " ' aagfa',\n",
       " ' aagfan',\n",
       " ' aagl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer_char_ngram.get_feature_names()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see lots of white spaces, which do not provide a good insight on the structure of the words. Although, silence is a important part of the music.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM CV + Bigrams 4 0.5910983725307404\n",
      "SVM CV + Bigrams macro 0.5789372643322429\n",
      "SVM CV + Bigrams 4 0.6773374236772861\n",
      "SVM CV + Bigrams macro 0.6700263168459002\n"
     ]
    }
   ],
   "source": [
    "def gather_bigrams(data):\n",
    "    res = set()\n",
    "    for n1 in data:\n",
    "        res.update(bigrams(n1))\n",
    "    return list(res)\n",
    "\n",
    "def bigrams(word):\n",
    "    chars = [c for c in word]\n",
    "    bigrams = [c1 + c2 for c1, c2 in zip(chars, chars[1:])]\n",
    "    features = chars + bigrams\n",
    "    return features\n",
    "\n",
    "def transform_features(data_train, data_test, n_grams=1):\n",
    "\n",
    "    bigrams_list = gather_bigrams([tj for tk in data_train for tj in tk.split() if tj.find(\" \")==-1])\n",
    "    cv = CountVectorizer(\n",
    "            analyzer=bigrams,\n",
    "       # analyzer=\"char\",\n",
    "            preprocessor=lambda x : x,\n",
    "            vocabulary=bigrams_list,\n",
    "        ngram_range=(1, n_grams))\n",
    "\n",
    "    \n",
    "    X_train = cv.fit_transform(data_train)\n",
    "\n",
    "    X_test = cv.transform(data_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "X_train, X_test = transform_features(sents_train,sents_dev)\n",
    "\n",
    "clf_svm.fit(X_train,labels_nr_train)\n",
    "preds=clf_svm.predict(X_test)\n",
    "print(\"SVM CV + Bigrams 4\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM CV + Bigrams macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))\n",
    "clf_svm.fit(scipy.sparse.hstack([tfidf_train,X_train]),labels_nr_train)\n",
    "preds=clf_svm.predict(scipy.sparse.hstack([tfidf_dev.todense(),X_test]))\n",
    "print(\"SVM CV + Bigrams 4\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM CV + Bigrams macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although alone these simple bigram analyzer performs poorly in combination with tf-idf, it increases the macro f-1 considerably, 0.005 is what differentiate mostly between the places.\n",
    "What is the impact of using ngram instead of words for tf-idf (we keep the char bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF + Bigrams + word 7 ngram 5 0.678517820542825\n",
      "SVM TF-IDF + Bigrams + word 7 ngram macro 0.6686880145927554\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer_ngram = TfidfVectorizer( min_df=2,\n",
    "                                 norm=\"l2\",\n",
    "                                   max_features=n_features, ngram_range=(1, 7))\n",
    "tfidf_train_ngram = tfidf_vectorizer_ngram.fit_transform(sents_train)\n",
    "tfidf_dev_ngram = tfidf_vectorizer_ngram.transform(sents_dev)\n",
    " \n",
    "clf_svm.fit(scipy.sparse.hstack([tfidf_train_ngram,X_train]),labels_nr_train)\n",
    "preds=clf_svm.predict(scipy.sparse.hstack([tfidf_dev_ngram,X_test]))\n",
    "print(\"SVM TF-IDF + Bigrams + word 7 ngram 5\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF-IDF + Bigrams + word 7 ngram macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So less macro and more weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF + Bigrams  + char 7 ngram 6 0.678517820542825\n",
      "SVM TF-IDF + Bigrams  + char 7 ngram macro 0.6686880145927554\n"
     ]
    }
   ],
   "source": [
    "X_train_ngrams, X_test_ngrams = transform_features(sents_train,sents_dev, n_grams=7)\n",
    "\n",
    "clf_svm.fit(scipy.sparse.hstack([tfidf_train_ngram,X_train_ngrams]),labels_nr_train)\n",
    "preds=clf_svm.predict(scipy.sparse.hstack([tfidf_dev_ngram,X_test_ngrams]))\n",
    "print(\"SVM TF-IDF + Bigrams  + char 7 ngram 6\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF-IDF + Bigrams  + char 7 ngram macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So no changes. Let's try to put even more features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF + Bigrams  + char 7 ngram (bigrams) tfidf chars ngrams + tf char ngram 7 0.6807815919337346\n",
      "SVM TF-IDF + Bigrams  + char 7 ngram (bigrams) tfidf chars ngrams + tf char ngram macro 0.6708978004436262\n"
     ]
    }
   ],
   "source": [
    "clf_svm.fit(scipy.sparse.hstack([tfidf_train_ngram,X_train_ngrams,tfidf_train_char_ngram]),labels_nr_train)\n",
    "preds=clf_svm.predict(scipy.sparse.hstack([tfidf_dev_ngram,X_test_ngrams,tfidf_dev_char_ngram]))\n",
    "print(\"SVM TF-IDF + Bigrams  + char 7 ngram (bigrams) tfidf chars ngrams + tf char ngram 7\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF-IDF + Bigrams  + char 7 ngram (bigrams) tfidf chars ngrams + tf char ngram macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, more for both features.\n",
    "So lets try normalizing the tf and using char ngram count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF + Bigrams  + char 7 ngram (bigrams) tfidf chars ngrams + tf char ngram 8 0.6817307419317699\n",
      "SVM TF-IDF + Bigrams  + char 7 ngram (bigrams) tfidf chars ngrams + tf char ngram macro 0.6725325917465754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "tf_vectorizer_char_ngram = CountVectorizer( min_df=2,\n",
    "                                               analyzer=\"char\",\n",
    "                                   max_features=n_features, ngram_range=(1, 7))\n",
    "tf_train_char_ngram = tf_vectorizer_char_ngram.fit_transform(sents_train)\n",
    "tf_dev_char_ngram = tf_vectorizer_char_ngram.transform(sents_dev)\n",
    "\n",
    "tf_train_char_ngram = normalize(tf_train_char_ngram)\n",
    "tf_dev_char_ngram = normalize(tf_dev_char_ngram)\n",
    "\n",
    "\n",
    "clf_svm.fit(scipy.sparse.hstack([tfidf_train_ngram,X_train_ngrams,tfidf_train_char_ngram,tf_train_char_ngram]),labels_nr_train)\n",
    "preds=clf_svm.predict(scipy.sparse.hstack([tfidf_dev_ngram,X_test_ngrams,tfidf_dev_char_ngram,tf_dev_char_ngram]))\n",
    "print(\"SVM TF-IDF + Bigrams  + char 7 ngram (bigrams) tfidf chars ngrams + tf char ngram 8\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF-IDF + Bigrams  + char 7 ngram (bigrams) tfidf chars ngrams + tf char ngram macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not much difference but still increased.\n",
    "Until now we just concatenated the features, but they are quite different, and can cause the SVMs to give more important to heavier features. A possible counter action would be to normalize it. Yet, since we have many different features and many different possible ways to normalize (over train, over feature over sample), we will use separate svms and use voting (ensemble voting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF separate svm for features Bigrams   char 7 ngram 9 0.6894441553644064\n",
      "SVM TF-IDF separate svm for features Bigrams   char 7 ngram macro 0.6758401343396775\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svms=[]\n",
    "for tk in [tfidf_train_ngram,X_train_ngrams,tfidf_train_char_ngram,tf_train_char_ngram]:\n",
    "    svms.append(LinearSVC(random_state=0, C=1))\n",
    "    svms[-1].fit(tk,labels_nr_train)\n",
    "\n",
    "svms_preds=[]\n",
    "for i1,tk in enumerate([tfidf_dev_ngram,X_test_ngrams,tfidf_dev_char_ngram,tf_dev_char_ngram]):\n",
    "    svms_preds.append(svms[i1].predict(tk))\n",
    "\n",
    "\n",
    "    \n",
    "#sumpres=sum([tk for tk in svms_preds])\n",
    "sumpres=[Counter(np.array(svms_preds)[:,tk]).most_common()[0][0] for tk in range(len(svms_preds[0]))]\n",
    "print(\"SVM TF-IDF separate svm for features Bigrams   char 7 ngram 9\",f1_score(sumpres,labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF-IDF separate svm for features Bigrams   char 7 ngram macro\",f1_score(sumpres,labels_nr_dev, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, not bad, almost one point in weighted, but again, the vote is not weighted. So we count each vote equal, although the classifier might be not so convinced of the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM TF-IDF separate svm for features Bigrams   char 7 ngram 10 0.6970817681702751\n",
      "SVM TF-IDF separate svm for features Bigrams   char 7 ngram macro 0.687829000954753\n"
     ]
    }
   ],
   "source": [
    "svms=[]\n",
    "for tk in [tf_train, tfidf_train, tfidf_train_ngram,X_train_ngrams,tfidf_train_char_ngram,tf_train_char_ngram]:\n",
    "    svms.append(LinearSVC(random_state=0, C=1))\n",
    "    svms[-1].fit(tk,labels_nr_train)\n",
    "\n",
    "svms_preds=[]\n",
    "for i1,tk in enumerate([tf_dev, tfidf_dev,tfidf_dev_ngram,X_test_ngrams,tfidf_dev_char_ngram,tf_dev_char_ngram]):\n",
    "    svms_preds.append(svms[i1].decision_function(tk))\n",
    "    \n",
    "sumpres=sum([tk for tk in svms_preds])\n",
    "print(\"SVM TF-IDF separate svm for features Bigrams   char 7 ngram 10\",f1_score(np.argmax(sumpres,1),labels_nr_dev, average=\"weighted\"))\n",
    "print(\"SVM TF-IDF separate svm for features Bigrams   char 7 ngram macro\",f1_score(np.argmax(sumpres,1),labels_nr_dev, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. almost 0.70, we would have beaten nicely the first place of last year.\n",
    "Still, a sum seems poor... can be build a meta classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta SVM TF-IDF separate svm for features Bigrams   char 7 ngram 0.6678147793590107\n"
     ]
    }
   ],
   "source": [
    "svms=[]\n",
    "for tk in [tfidf_train_ngram,X_train_ngrams,tfidf_train_char_ngram,tf_train_char_ngram]:\n",
    "    svms.append(LinearSVC(random_state=0, C=1))\n",
    "    svms[-1].fit(tk,labels_nr_train)\n",
    "\n",
    "svms_train_meta=[]\n",
    "for i1,tk in enumerate([tfidf_train_ngram,X_train_ngrams,tfidf_train_char_ngram,tf_train_char_ngram]):\n",
    "    svms_train_meta.append(svms[i1].decision_function(tk))\n",
    "\n",
    "svm_meta=LinearSVC(random_state=0, C=0.75)\n",
    "svm_meta.fit(scipy.sparse.hstack([np.concatenate(svms_train_meta,1),tfidf_train_ngram,X_train_ngrams,tfidf_train_char_ngram,tf_train_char_ngram]),labels_nr_train)\n",
    "\n",
    "\n",
    "\n",
    "svms_preds=[]\n",
    "for i1,tk in enumerate([tfidf_dev_ngram,X_test_ngrams,tfidf_dev_char_ngram,tf_dev_char_ngram]):\n",
    "    svms_preds.append(svms[i1].decision_function(tk))\n",
    "svm_meta_preds=svm_meta.decision_function(scipy.sparse.hstack([np.concatenate(svms_preds,1),tfidf_dev_ngram,X_test_ngrams,tfidf_dev_char_ngram,tf_dev_char_ngram]))\n",
    "\n",
    "\n",
    "print(\"Meta SVM TF-IDF separate svm for features Bigrams   char 7 ngram\",f1_score(np.argmax(svm_meta_preds,1),labels_nr_dev, average=\"macro\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was not good, probably the classifiers did too well in the training set. The solution is to do a cross validation, i.e. slice the training data e.g. in 5 slices, use one slice for test and the others for training, and iterate, switching the test slice every time (5 times in total). We make predictions on the test slice, gathering all predictions creates a prediction set which we can train a meta classifier. I did not refactor the code, so it can be quite long to go through but that is the main idea. We use the helper function perform from meta_cv. Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp 0\n",
      "SVM TF 0.666387461900244\n",
      "SVM TF macro 0.6564319851674072\n"
     ]
    }
   ],
   "source": [
    "imp.reload(meta_cv)\n",
    "import meta_cv\n",
    "from meta_cv import MetaCV\n",
    "\n",
    "#download gold labels\n",
    "gold_dir=\"./data/gold/\"\n",
    "if not os.path.exists(gold_dir):\n",
    "    os.makedirs(gold_dir)\n",
    "target_path_gold=download_file(\"https://drive.google.com/uc?authuser=0&id=0B8I6bgWbt_MfWHJrNDhpOFVVcUhabGFxWDJUWG9LR2hGODFJ&export=download\", target_path=gold_dir+\"testdata_fixed.zip\")\n",
    "extract_zip( target_path_gold, gold_dir)\n",
    "\n",
    "def perform(comp):\n",
    "    meta=meta_cv.MetaCV(splits=5)\n",
    "    print (\"comp\",comp)\n",
    "    labels_train, labels_nr_train, labels_dict_train,sents_train_raw, words_train, chars_train, char_counts_train = get_data()\n",
    "    labels_dev_dev, labels_nr_dev, labels_dict_dev,sents_dev_raw, words_dev, chars_dev, char_counts_dev = get_data(fname=\"./data/dev.txt\")\n",
    "    sents_train= [\" \".join(tk).lower() for tk in sents_train_raw]\n",
    "    sents_dev= [\" \".join(tk).lower() for tk in sents_dev_raw]\n",
    "    labels_dev_dev, labels_nr_dev, labels_dict_dev,sents_dev_raw, words_dev, chars_dev, char_counts_dev = get_data(fname=\"./data/dev.txt\")\n",
    "        \n",
    "    labels_test, labels_nr_test, labels_dict_test,sents_test_raw, words_test, chars_test, char_counts_test = get_data(fname=\"./data/gold/gold.txt\")\n",
    "\n",
    "    if comp==0:\n",
    "        meta.fit(sents_train,labels_nr_train)\n",
    "        preds=meta.predict(sents_dev)\n",
    "        print(\"SVM TF\",f1_score(preds,labels_nr_dev, average=\"weighted\"))\n",
    "        print(\"SVM TF macro\",f1_score(preds,labels_nr_dev, average=\"macro\"))\n",
    "    else:\n",
    "        rev_labels_dict_train = dict([(tv,tk) for tk,tv in labels_dict_train.items()])\n",
    "        sents_train_comp= [\" \".join(tk).lower() for tj in [sents_train_raw,sents_dev_raw] for tk in tj]\n",
    "        labels_nr_train_comp = [ tk for tj in [labels_nr_train,labels_nr_dev] for tk in tj]\n",
    "        meta.fit(sents_train_comp,labels_nr_train_comp) \n",
    "        sents_test_comp= [\" \".join(tk).lower() for tk in sents_test_raw]\n",
    "        preds=meta.predict(sents_test_comp)\n",
    "        labels_preds=[rev_labels_dict_train[np.argmax(tk)] for tk in preds]\n",
    "        with open(\"prediction.labels\",\"w\") as f1:\n",
    "            for label in labels_preds:\n",
    "                f1.write(label+\"\\n\")\n",
    "\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "\n",
    "        preds_score=meta.decision_function(sents_test_comp)\n",
    "\n",
    "        trf=np.argmax(preds_score,1)\n",
    "\n",
    "        rev_labels_dict_train=dict([(tv,tk) for tk,tv in labels_dict_train.items()])\n",
    "        rev_labels_dict_train[-1]=\"XY\"\n",
    "\n",
    "        tr_labs=[rev_labels_dict_train[trf[tk]] for tk in range(trf.shape[0])] \n",
    "\n",
    "        with open(\"predictions_c5_metacv_multiclass_threshold.labels\",\"w\") as f1:\n",
    "            for tr1 in tr_labs:\n",
    "                f1.write(tr1+\"\\n\")\n",
    "\n",
    "        gdi4=np.isin(labels_nr_test,[labels_dict_test[tk] for tk in [\"ZH\",\"LU\",\"BE\",\"BS\"]])\n",
    "        index=np.where(gdi4)[0]\n",
    "\n",
    "        print(\"SVM TF\",f1_score([labels_dict_test[rev_labels_dict_train[tk]] for tk in np.array(trf)[index]],np.array(labels_nr_test)[index], average=\"weighted\"))\n",
    "        print(\"SVM TF macro\",f1_score([labels_dict_test[rev_labels_dict_train[tk]] for tk in np.array(trf)[index]],np.array(labels_nr_test)[index], average=\"macro\"))\n",
    "\n",
    "        return index, gdi4,trf, labels_dict_test,rev_labels_dict_train,labels_nr_test, preds_score\n",
    "\n",
    "perform(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, now we are set to compete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uall 0 comp 1 tf 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-941ec1045791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmeta_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/projects/competitions/Vardial_blog/meta_cv.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(uall, comp, tf)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m#if \"labels_nr_train\" not in globals():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muall\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_nr_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msents_train_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_counts_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mlabels_dev_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_nr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msents_dev_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_counts_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dev_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0msents_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents_train_raw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "import bigrams\n",
    "meta_cv.perform(0,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With only one feature, the winner did win with multiple features, and they were quite good also before. Can we mix them somehow and get better results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(meta_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
